{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This post contains some notes about three Python libraries for working with numerical data too large to fit into main memory: [h5py](http://www.h5py.org/), [Bcolz](http://bcolz.blosc.org/) and [Zarr](https://github.com/alimanfoo/zarr).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDF5 (``h5py``)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I first discovered the [HDF5 file format](https://www.hdfgroup.org/HDF5/) a few years ago it was pretty transformative. I'd been struggling to find efficient ways of exploring and analysing data coming from [our research](http://www.malariagen.net/ag1000g) on genetic variation in the mosquitoes that carry malaria. The data are not enormous - typically integer arrays with around 20 billion elements - but they are too large to work with in memory on a typical laptop or desktop machine. The file formats traditionally used for these data are very slow to parse, so I went looking for an alternative.\n",
    "\n",
    "HDF5 files provide a great solution for storing multi-dimensional arrays of numerical data. The arrays are divided up into chunks and each chunk is compressed, enabling data to be stored efficiently in memory or on disk. Depending on how the chunks are configured, usually a good compromise can be achieved that means data can be read very quickly even when using different access patterns, e.g., taking horizontal or vertical slices of a matrix. Also, the [``h5py``](http://www.h5py.org/) Python library provides a very convenient API for working with HDF5 files. I found there was a whole range of analyses I could happily get done on my laptop on the train home from work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bcolz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit later on I discovered the [Bcolz](http://bcolz.blosc.org/) library. Bcolz is primarily intended for storing and querying large tables of data, but it does provide a [``carray``](http://bcolz.blosc.org/reference.html#the-carray-class) class that is roughly analogous to an HDF5 dataset in that it can store numerical arrays in a chunked, compressed form, either in memory or on disk. \n",
    "\n",
    "Reading and writing data to a ``bcolz.carray`` is typically a lot faster than HDF5. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import bcolz\n",
    "import tempfile\n",
    "\n",
    "\n",
    "def h5fmem(**kwargs):\n",
    "    \"\"\"Convenience function to create an in-memory HDF5 file.\"\"\"\n",
    "\n",
    "    # need a file name even tho nothing is ever written\n",
    "    fn = tempfile.mktemp()\n",
    "\n",
    "    # file creation args\n",
    "    kwargs['mode'] = 'w'\n",
    "    kwargs['driver'] = 'core'\n",
    "    kwargs['backing_store'] = False\n",
    "\n",
    "    # open HDF5 file\n",
    "    h5f = h5py.File(fn, **kwargs)\n",
    "\n",
    "    return h5f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup a simple array of integer data to store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([       0,        1,        2, ..., 99999997, 99999998, 99999999], dtype=int32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1 = np.arange(1e8, dtype='i4')\n",
    "a1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time how long it takes to store in an HDF5 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 1.72 s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit h5fmem().create_dataset('arange', data=a1, chunks=(2**18,), compression='gzip', compression_opts=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time how long it takes to store in a ``bcolz.carray``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 69.6 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit bcolz.carray(a1, chunklen=2**18, cparams=bcolz.cparams(cname='lz4', clevel=5, shuffle=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, Bcolz is more than 10 times faster at storing (compressing) the data than HDF5. As I understand it, this performance gain comes from several factors. Bcolz uses a C library called [Blosc](https://github.com/blosc/c-blosc) to perform compression and decompression operations. Blosc can use multiple threads internally, so some of the work is done in parallel. Blosc also splits data up in a way that is designed to work well with the CPU cache architecture. Finally, Blosc is a meta-compressor and several different compression libraries can be used internally - above I used the LZ4 compressor, which does not achieve quite the same compression ratios as gzip (zlib) but is much faster with numerical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speed really makes a difference when working interactively with data, so I started using the ``bcolz.carray`` class where possible in my analyses, especially for storing intermediate data. However, it does have some limitations. A ``bcolz.carray`` can be multidimensional, but because Bcolz is not really designed for multi-dimensional data, a ``bcolz.carray`` can only be chunked along the first dimension. This means taking slices of the first dimension is efficient, but slicing any other dimension will be very inefficient, because the entire array will need to be read and decompressed to access even a single column of a matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explore better ways of working with large multi-dimensional data, I recently created a new library called [Zarr](https://github.com/alimanfoo/zarr). Zarr like Bcolz uses Blosc internally to handle all compression and decompression operations. However, Zarr supports chunking of arrays along multiple dimensions, enabling good performance for multiple data access patterns. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.3'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import zarr\n",
    "zarr.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup a 2-dimensional array of integer data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a2 = np.arange(1e8, dtype='i4').reshape(10000, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the data in a ``carray``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "carray((10000, 10000), int32)\n",
       "  nbytes: 381.47 MB; cbytes: 10.63 MB; ratio: 35.87\n",
       "  cparams := cparams(clevel=5, shuffle=1, cname='blosclz')\n",
       "[[       0        1        2 ...,     9997     9998     9999]\n",
       " [   10000    10001    10002 ...,    19997    19998    19999]\n",
       " [   20000    20001    20002 ...,    29997    29998    29999]\n",
       " ..., \n",
       " [99970000 99970001 99970002 ..., 99979997 99979998 99979999]\n",
       " [99980000 99980001 99980002 ..., 99989997 99989998 99989999]\n",
       " [99990000 99990001 99990002 ..., 99999997 99999998 99999999]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2 = bcolz.carray(a2, chunklen=100)\n",
    "c2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the data in a ``zarr`` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((10000, 10000), int32, chunks=(1000, 1000), order=C)\n",
       "  nbytes: 381.5M; nbytes_stored: 9.2M; ratio: 41.6; initialized: 100/100\n",
       "  compressor: Blosc(cname='lz4', clevel=5, shuffle=1)\n",
       "  store: dict"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = zarr.array(a2, chunks=(1000, 1000))\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time how long it takes to access a slice along the first dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 11 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit c2[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 12.1 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit z[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time how long it takes to access a slice along the second dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 115 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit c2[:, :1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 10.1 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit z[:, :1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using Zarr and chunking along both dimensions of the array, we have forfeited a small amount of speed when slicing the first dimension to gain a lot of speed when accessing the second dimension.\n",
    "\n",
    "Like h5py and Bcolz, Zarr can store data either in memory or on disk. Zarr has some other notable features too. For example, multi-dimensional arrays can be resized along any dimension, allowing an array to be grown by appending new data in a flexible way. Also, [Zarr arrays can be used in parallel computations](http://alimanfoo.github.io/2016/05/16/cpu-blues.html), supporting concurrent reads and writes in either a multi-threaded or multi-process context. \n",
    "\n",
    "Zarr is still in an experimental phase, but if you do try it out, any feedback is very welcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [HDF5](https://www.hdfgroup.org/HDF5/)\n",
    "* [h5py](http://www.h5py.org/)\n",
    "* [Bcolz](http://bcolz.blosc.org/)\n",
    "* [Blosc](http://blosc.org/)\n",
    "* [Zarr](https://github.com/alimanfoo/zarr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-script: Performance with real genotype data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For detailed benchmark data with Zarr, see [genotype compressor benchmark](http://alimanfoo.github.io/2016/09/21/genotype-compression-benchmark.html).\n",
    "\n",
    "Here are a few simple benchmarks with some real genotype data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "def human_readable_size(size):\n",
    "    if size < 2**10:\n",
    "        return \"%s\" % size\n",
    "    elif size < 2**20:\n",
    "        return \"%.1fK\" % (size / float(2**10))\n",
    "    elif size < 2**30:\n",
    "        return \"%.1fM\" % (size / float(2**20))\n",
    "    elif size < 2**40:\n",
    "        return \"%.1fG\" % (size / float(2**30))\n",
    "    else:\n",
    "        return \"%.1fT\" % (size / float(2**40))\n",
    "\n",
    "    \n",
    "def h5d_diagnostics(d):\n",
    "    \"\"\"Print some diagnostics on an HDF5 dataset.\"\"\"\n",
    "    \n",
    "    print(d)\n",
    "    nbytes = reduce(operator.mul, d.shape) * d.dtype.itemsize\n",
    "    cbytes = d._id.get_storage_size()\n",
    "    if cbytes > 0:\n",
    "        ratio = nbytes / cbytes\n",
    "    else:\n",
    "        ratio = np.inf\n",
    "    r = '  cname=%s' % d.compression\n",
    "    r += ', clevel=%s' % d.compression_opts\n",
    "    r += ', shuffle=%s' % d.shuffle\n",
    "    r += '\\n  nbytes=%s' % human_readable_size(nbytes)\n",
    "    r += ', cbytes=%s' % human_readable_size(cbytes)\n",
    "    r += ', ratio=%.1f' % ratio\n",
    "    r += ', chunks=%s' % str(d.chunks)\n",
    "    print(r)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Locate a genotype dataset within an HDF5 file from the [Ag1000G project](http://www.malariagen.net/data/ag1000g-phase1-AR3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"genotype\": shape (13167162, 765, 2), type \"|i1\">"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callset = h5py.File('data/2016-04-14/ag1000g.phase1.ar3.pass.h5', mode='r')\n",
    "genotype = callset['3R/calldata/genotype']\n",
    "genotype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the first million rows of the dataset to use for benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a3 = genotype[:1000000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmark compression performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.17 s, sys: 0 ns, total: 9.17 s\n",
      "Wall time: 9.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "genotype_hdf5_gzip = h5fmem().create_dataset('genotype', data=a3, \n",
    "                                             compression='gzip', compression_opts=1, shuffle=False,\n",
    "                                             chunks=(10000, 100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"genotype\": shape (1000000, 765, 2), type \"|i1\">\n",
      "  cname=gzip, clevel=1, shuffle=False\n",
      "  nbytes=1.4G, cbytes=51.1M, ratio=28.5, chunks=(10000, 100, 2)\n"
     ]
    }
   ],
   "source": [
    "h5d_diagnostics(genotype_hdf5_gzip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.67 s, sys: 0 ns, total: 1.67 s\n",
      "Wall time: 1.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "genotype_hdf5_lzf = h5fmem().create_dataset('genotype', data=a3, \n",
    "                                             compression='lzf', shuffle=False,\n",
    "                                             chunks=(10000, 100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"genotype\": shape (1000000, 765, 2), type \"|i1\">\n",
      "  cname=lzf, clevel=None, shuffle=False\n",
      "  nbytes=1.4G, cbytes=71.7M, ratio=20.3, chunks=(10000, 100, 2)\n"
     ]
    }
   ],
   "source": [
    "h5d_diagnostics(genotype_hdf5_lzf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.72 s, sys: 0 ns, total: 2.72 s\n",
      "Wall time: 499 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "genotype_carray = bcolz.carray(a3, cparams=bcolz.cparams(cname='lz4', clevel=1, shuffle=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "carray((1000000, 765, 2), int8)\n",
       "  nbytes: 1.42 GB; cbytes: 48.70 MB; ratio: 29.96\n",
       "  cparams := cparams(clevel=1, shuffle=2, cname='lz4')\n",
       "[[[0 0]\n",
       "  [0 0]\n",
       "  [0 0]\n",
       "  ..., \n",
       "  [0 0]\n",
       "  [0 0]\n",
       "  [0 0]]\n",
       "\n",
       " [[0 0]\n",
       "  [0 0]\n",
       "  [0 0]\n",
       "  ..., \n",
       "  [0 0]\n",
       "  [0 0]\n",
       "  [0 0]]\n",
       "\n",
       " [[0 0]\n",
       "  [0 0]\n",
       "  [0 0]\n",
       "  ..., \n",
       "  [0 0]\n",
       "  [0 0]\n",
       "  [0 0]]\n",
       "\n",
       " ..., \n",
       " [[0 0]\n",
       "  [0 0]\n",
       "  [0 0]\n",
       "  ..., \n",
       "  [0 0]\n",
       "  [0 0]\n",
       "  [0 0]]\n",
       "\n",
       " [[0 0]\n",
       "  [0 0]\n",
       "  [0 0]\n",
       "  ..., \n",
       "  [0 0]\n",
       "  [0 0]\n",
       "  [0 0]]\n",
       "\n",
       " [[0 0]\n",
       "  [0 0]\n",
       "  [0 0]\n",
       "  ..., \n",
       "  [0 0]\n",
       "  [0 0]\n",
       "  [0 0]]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genotype_carray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.32 s, sys: 60 ms, total: 2.38 s\n",
      "Wall time: 744 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "genotype_zarr = zarr.array(a3, chunks=(10000, 100, 2), compression='blosc', \n",
    "                           compression_opts=dict(cname='lz4', clevel=1, shuffle=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((1000000, 765, 2), int8, chunks=(10000, 100, 2), order=C)\n",
       "  nbytes: 1.4G; nbytes_stored: 49.0M; ratio: 29.8; initialized: 800/800\n",
       "  compressor: Blosc(cname='lz4', clevel=1, shuffle=2)\n",
       "  store: dict"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genotype_zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that although I've used the LZ4 compression library with Bcolz and Zarr, the compression ratio is actually better than when using gzip (zlib) with HDF5. This is due to the bitshuffle filter, which comes bundled with Blosc. The bitshuffle filter can also be used with HDF5 with some configuration I believe.\n",
    "\n",
    "Compression with Zarr is slightly slower than Bcolz above, but this is entirely due to the choice of chunk shape and the correlation structure in the data. If we use the same chunking for both, compression speed is similar..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.69 s, sys: 96 ms, total: 1.78 s\n",
      "Wall time: 338 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "_ = zarr.array(a3, chunks=(genotype_carray.chunklen, 765, 2), compression='blosc',\n",
    "               compression_opts=dict(cname='lz4', clevel=1, shuffle=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmark data access via slices along first and second dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 77.7 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit genotype_hdf5_gzip[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 20.2 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit genotype_hdf5_lzf[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 7.61 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit genotype_carray[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 9.16 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit genotype_zarr[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 972 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit genotype_hdf5_gzip[:, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 208 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit genotype_hdf5_lzf[:, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 919 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit genotype_carray[:, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 60.7 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit genotype_zarr[:, :10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture:          x86_64\r\n",
      "CPU op-mode(s):        32-bit, 64-bit\r\n",
      "Byte Order:            Little Endian\r\n",
      "CPU(s):                8\r\n",
      "On-line CPU(s) list:   0-7\r\n",
      "Thread(s) per core:    2\r\n",
      "Core(s) per socket:    4\r\n",
      "Socket(s):             1\r\n",
      "NUMA node(s):          1\r\n",
      "Vendor ID:             GenuineIntel\r\n",
      "CPU family:            6\r\n",
      "Model:                 94\r\n",
      "Model name:            Intel(R) Xeon(R) CPU E3-1505M v5 @ 2.80GHz\r\n",
      "Stepping:              3\r\n",
      "CPU MHz:               1563.515\r\n",
      "CPU max MHz:           3700.0000\r\n",
      "CPU min MHz:           800.0000\r\n",
      "BogoMIPS:              5615.87\r\n",
      "Virtualisation:        VT-x\r\n",
      "L1d cache:             32K\r\n",
      "L1i cache:             32K\r\n",
      "L2 cache:              256K\r\n",
      "L3 cache:              8192K\r\n",
      "NUMA node0 CPU(s):     0-7\r\n",
      "Flags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx rdseed adx smap clflushopt xsaveopt xsavec xgetbv1 dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp\r\n"
     ]
    }
   ],
   "source": [
    "!lscpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-10-18T11:46:22.554534\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(datetime.datetime.now().isoformat())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
